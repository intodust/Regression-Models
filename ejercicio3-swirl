| Please choose a course, or type 0 to exit swirl.

1: Regression Models
2: Take me to the swirl course repository!

Selection: 1

| Please choose a lesson, or type 0 to return to course menu.

 1: Introduction                               2: Residuals                               
 3: Least Squares Estimation                   4: Residual Variation                      
 5: Introduction to Multivariable Regression   6: MultiVar Examples                       
 7: MultiVar Examples2                         8: MultiVar Examples3                      
 9: Residuals Diagnostics and Variation       10: Variance Inflation Factors              
11: Overfitting and Underfitting              12: Binary Outcomes                         
13: Count Outcomes                            

Selection: 3
  |                                                                                                                    |   0%

| Least Squares Estimation. (Slides for this and other Data Science courses may be found at github
| https://github.com/DataScienceSpecialization/courses. If you care to use them, they must be downloaded as a zip file and
| viewed locally. This lesson corresponds to Regression_Models/01_03_ols. Galton data is from John Verzani's website,
| http://wiener.math.csi.cuny.edu/UsingR/)

...

  |======                                                                                                              |   6%

| In this lesson, if you're using RStudio, you'll be able to play with some of the code which appears in the slides. If
| you're not using RStudio, you can look at the code but you won't be able to experiment with the function "manipulate". We
| provide the code for you so you can examine it without having to type it all out.  In RStudio, when the edit window
| displays code, make sure your flashing cursor is back in the console window before you hit "Enter" or any keyboard buttons,
| otherwise you might accidentally alter the code. If you do alter the file, in RStudio, you can hit Ctrl z in the editor
| until all the unwanted changes disappear. In other editors, you'll have to use whatever key combination performs "undo" to
| remove all your unwanted changes.

...

  |=============                                                                                                       |  11%

| Here are the Galton data and the regression line seen in the Introduction. The regression line summarizes the relationship
| between parents' heights (the predictors) and their childrens' (the outcomes).

...

  |===================                                                                                                 |  17%

| We learned in the last lesson that the regression line is the line through the data which has the minimum (least) squared
| "error", the vertical distance between the 928 actual children's heights and the heights predicted by the line. Squaring
| the distances ensures that data points above and below the line are treated the same. This method of choosing the 'best'
| regression line (or 'fitting' a line to the data) is known as ordinary least squares.

...

  |==========================                                                                                          |  22%

| As shown in the slides, the regression line contains the point representing the means of the two sets of heights. These are
| shown by the thin horizontal and vertical lines. The intersection point is shown by the triangle on the plot. Its
| x-coordinate is the mean of the parents' heights and y-coordinate is the mean of the childrens' heights.

...

  |================================                                                                                    |  28%

| As shown in the slides, the slope of the regression line is the correlation between the two sets of heights multiplied by
| the ratio of the standard deviations (childrens' to parents' or outcomes to predictors).

...

  |=======================================                                                                             |  33%

| Here we show code which demonstrates how changing the slope of the regression line affects the mean squared error between
| actual and predicted values. Look it over to see how straightforward it is.

...

  |=============================================                                                                       |  39%

| What RStudio graphics package allows the user to play with the data to see the effects of the changes?

1: manipulate
2: plot
3: abline
4: points

Selection: 1

| All that practice is paying off!

  |====================================================                                                                |  44%

| Now you can actually play with the code to use R's manipulate function and find the minimum squared error. You can adjust
| the slider with the left mouse button or use the right and left arrow keys to see how changing the slope (beta) affects the
| mean squared error (mse). If the slider disappears you can call it back by clicking on the little gear in the upper left
| corner of the plot window.

...

  |==========================================================                                                          |  50%

| Which value of the slope minimizes the mean squared error?

1: .64
2: .44
3: 5
4: .70

Selection: 1

| Excellent job!

  |================================================================                                                    |  56%

| What was the minimum mse?

1: .66
2: .64
3: 44
4: 5.0

Selection: 4

| You are really on a roll!

  |=======================================================================                                             |  61%

| Recall that you normalize data by subtracting its mean and dividing by its standard deviation. We've done this for the
| galton child and parent data for you. We've stored these normalized values in two vectors, gpa_nor and gch_nor, the
| normalized galton parent and child data.

...

  |=============================================================================                                       |  67%

| Use R's function "cor" to compute the correlation between these normalized data sets.

> cor(gpa_nor,gch_nor)
[1] 0.4587624

| You're the best!

  |====================================================================================                                |  72%

| How does this correlation relate to the correlation of the unnormalized data?

1: It is smaller.
2: It is bigger.
3: It is the same.

Selection: 3

| Your dedication is inspiring!

  |==========================================================================================                          |  78%

| Use R's function "lm" to generate the regression line using this normalized data. Store it in a variable called l_nor. Use
| the parents' heights as the predictors (independent variable) and the childrens' as the predicted (dependent). Remember,
| 'lm' needs a formula of the form dependent ~ independent. Since we've created the data vectors for you there's no need to
| provide a second "data" argument as you have previously.

> l_nor <- lm(gch_nor ~ gpa_nor)

| Keep up the great work!

  |=================================================================================================                   |  83%

| What is the slope of this line?

1: 1.
2: The correlation of the 2 data sets
3: I have no idea

Selection: 2

| All that practice is paying off!

  |=======================================================================================================             |  89%

| If you swapped the outcome (Y) and predictor (X) of your original (unnormalized) data, (for example, used childrens'
| heights to predict their parents), what would the slope of the new regression line be?

1: 1.
2: the same as the original
3: correlation(X,Y) * sd(X)/sd(Y)
4: I have no idea

Selection: 3

| Excellent job!

  |==============================================================================================================      |  94%

| We'll close with a final display of source code from the slides. It plots the galton data with three regression lines, the
| original in red with the children as the outcome, a new blue line with the parents' as outcome and childrens' as predictor,
| and a black line with the slope scaled so it equals the ratio of the standard deviations.

...

  |====================================================================================================================| 100%

| Congrats! You've concluded this lesson on ordinary least squares which are truly extraordinary!

...

| Are you currently enrolled in the Coursera course associated with this lesson?

1: Yes
2: No

Selection: 1

| Would you like me to notify Coursera that you've completed this lesson? If so, I'll need to get some more info from you.

1: Yes
2: No
3: Maybe later

Selection: 1

| Is the following information correct?

Course ID: regmods-016
Submission login (email): 
Submission password: 

1: Yes, go ahead!
2: No, I need to change something.

Selection: 1

| I'll try to tell Coursera you've completed this lesson now.

| Great work!

| I've notified Coursera that you have completed regmods-016, Least_Squares_Estimation.

| You've reached the end of this lesson! Returning to the main menu...